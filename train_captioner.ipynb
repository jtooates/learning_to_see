{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Vision Captioner on Synthetic Scenes\n",
    "\n",
    "This notebook trains a ConvNeXt-Tiny + GRU captioner with FSM-constrained decoding on synthetic scene data.\n",
    "\n",
    "**Hardware**: Designed for Google Colab with A100 GPU\n",
    "\n",
    "**Architecture**:\n",
    "- Encoder: ConvNeXt-Tiny (8 blocks, 256-dim output)\n",
    "- Decoder: GRU with Bahdanau attention (512-dim hidden)\n",
    "- Training: AdamW + AMP + OneCycleLR + Scheduled Sampling\n",
    "- Decoding: FSM-constrained to ensure grammar compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's mount Google Drive and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision tqdm pillow scikit-learn matplotlib seaborn ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional, for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository from GitHub\nimport os\nimport sys\n\nGITHUB_REPO = \"https://github.com/jtooates/learning_to_see.git\"\n\nif not os.path.exists('/content/learning_to_see'):\n    print(\"üì• Cloning repository...\")\n    !git clone {GITHUB_REPO}\n    print(\"‚úÖ Repository cloned!\")\nelse:\n    print(\"‚úÖ Repository already exists\")\n\n# Change to project directory\n%cd /content/learning_to_see\n\n# Add to Python path\nsys.path.insert(0, '/content/learning_to_see')\n\nprint(f\"\\nüìÇ Current directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training Data\n",
    "\n",
    "Generate synthetic scenes with images and captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate training data\n# This creates 6000 total samples, split 80/10/10 for train/val/test\n!python -m data.gen \\\n    --out_dir ./data/scenes \\\n    --n 6000 \\\n    --split_strategy random \\\n    --seed 42\n\nprint(\"\\n‚úÖ Data generation complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Data\n",
    "\n",
    "Let's look at some examples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from data.dataset import SceneDataset\nfrom dsl.tokens import Vocab\nimport os\n\n# Check if data exists\ndata_path = './data/scenes'\nif not os.path.exists(data_path):\n    print(f\"‚ö†Ô∏è  ERROR: Data directory not found at {data_path}\")\n    print(f\"   Please run the data generation cell above first!\")\n    raise FileNotFoundError(f\"Data directory {data_path} does not exist. Run cell 8 to generate data.\")\n\nif not os.path.exists(f'{data_path}/splits.json'):\n    print(f\"‚ö†Ô∏è  ERROR: Data not fully generated\")\n    print(f\"   The data generation command may have failed.\")\n    print(f\"   Please check the output of cell 8 for errors.\")\n    raise FileNotFoundError(f\"splits.json not found in {data_path}. Data generation incomplete.\")\n\n# Load vocab and dataset\nvocab = Vocab()\ntrain_dataset = SceneDataset(\n    data_dir=data_path,\n    split='train',\n    vocab=vocab\n)\n\nprint(f\"‚úì Training samples: {len(train_dataset)}\")\nprint(f\"‚úì Vocabulary size: {vocab.vocab_size}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "def show_samples(dataset, num_samples=8, cols=4):\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for idx, ax in zip(indices, axes):\n",
    "        sample = dataset[idx]\n",
    "        image = sample['image'].permute(1, 2, 0).numpy()\n",
    "        text = sample['text']\n",
    "        \n",
    "        ax.imshow(image)\n",
    "        ax.set_title(text, fontsize=10, wrap=True)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide extra axes\n",
    "    for ax in axes[num_samples:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_samples(train_dataset, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data Augmentations\n",
    "\n",
    "Show how augmentations transform the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captioner.augmentations import get_train_augmentation\n",
    "\n",
    "# Get a sample image\n",
    "sample = train_dataset[0]\n",
    "image = sample['image']\n",
    "text = sample['text']\n",
    "\n",
    "# Apply augmentation multiple times\n",
    "aug = get_train_augmentation(image_size=64, strong=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(image.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title('Original', fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Augmented versions\n",
    "for i in range(1, 8):\n",
    "    augmented = aug(image)\n",
    "    axes[i].imshow(augmented.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(f'Augmented {i}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "fig.suptitle(f'Caption: \"{text}\"', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "Create the captioner model and inspect its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captioner import build_captioner\n",
    "\n",
    "# Build model\n",
    "model = build_captioner(\n",
    "    vocab_size=vocab.vocab_size,\n",
    "    embed_dim=256,\n",
    "    hidden_dim=512,\n",
    "    encoder_dim=256,\n",
    "    attention_dim=256,\n",
    "    dropout=0.3,\n",
    "    drop_path_rate=0.1,\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel size: ~{total_params * 4 / 1024 / 1024:.1f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model architecture\n",
    "print(\"\\n=== MODEL ARCHITECTURE ===\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass\n",
    "\n",
    "Verify the model works with a small batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test forward pass\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = model.to(device)\n\n# Get a batch\nfrom data.dataset import create_dataloaders\n\ntrain_loader, val_loader, test_loader = create_dataloaders(\n    data_dir='./data/scenes',\n    vocab=vocab,\n    batch_size=4,\n    num_workers=2\n)\n\nbatch = next(iter(train_loader))\nimages = batch['image'].to(device)\ntargets = batch['input_ids'].to(device)\n\nprint(f\"Images shape: {images.shape}\")\nprint(f\"Targets shape: {targets.shape}\")\n\n# Forward pass\nmodel.eval()\nwith torch.no_grad():\n    logits, loss = model(images, targets, teacher_forcing_ratio=1.0)\n\nprint(f\"\\nLogits shape: {logits.shape}\")\nprint(f\"Loss: {loss.item():.4f}\")\nprint(\"\\n‚úì Forward pass successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Configure training parameters and create trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captioner.train import CaptionerTrainer\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_epochs': 50,\n",
    "    'warmup_epochs': 5,\n",
    "    'batch_size': 128,  # A100 can handle this\n",
    "    'use_amp': True,\n",
    "    'checkpoint_dir': './checkpoints',\n",
    "    'log_interval': 20\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create dataloaders with training batch size\ntrain_loader, val_loader, test_loader = create_dataloaders(\n    data_dir='./data/scenes',\n    vocab=vocab,\n    batch_size=config['batch_size'],\n    num_workers=4,\n    pin_memory=True\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")\nprint(f\"Test batches: {len(test_loader)}\")\nprint(f\"Total training steps: {len(train_loader) * config['max_epochs']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild model for training\n",
    "model = build_captioner(\n",
    "    vocab_size=vocab.vocab_size,\n",
    "    embed_dim=256,\n",
    "    hidden_dim=512,\n",
    "    encoder_dim=256,\n",
    "    attention_dim=256,\n",
    "    dropout=0.3,\n",
    "    drop_path_rate=0.1,\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = CaptionerTrainer(\n",
    "    model=model,\n",
    "    vocab=vocab,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    max_epochs=config['max_epochs'],\n",
    "    warmup_epochs=config['warmup_epochs'],\n",
    "    use_amp=config['use_amp'],\n",
    "    checkpoint_dir=config['checkpoint_dir'],\n",
    "    log_interval=config['log_interval']\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train the model and track metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# This will take ~30-60 minutes on A100 for 50 epochs\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Plot training and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# Load metrics from checkpoints\n",
    "checkpoint_dir = Path(config['checkpoint_dir'])\n",
    "metric_files = sorted(checkpoint_dir.glob('checkpoint_epoch_*_metrics.json'))\n",
    "\n",
    "epochs = []\n",
    "val_losses = []\n",
    "exact_matches = []\n",
    "token_accuracies = []\n",
    "color_f1s = []\n",
    "shape_f1s = []\n",
    "\n",
    "for f in metric_files:\n",
    "    epoch = int(f.stem.split('_')[2])\n",
    "    with open(f) as fp:\n",
    "        metrics = json.load(fp)\n",
    "    \n",
    "    epochs.append(epoch)\n",
    "    val_losses.append(metrics['val_loss'])\n",
    "    exact_matches.append(metrics['exact_match'])\n",
    "    token_accuracies.append(metrics['token_accuracy'])\n",
    "    color_f1s.append(metrics['color_f1'])\n",
    "    shape_f1s.append(metrics['shape_f1'])\n",
    "\n",
    "# Plot metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(epochs, val_losses, marker='o', linewidth=2, markersize=4)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Validation Loss')\n",
    "axes[0, 0].set_title('Validation Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Exact Match\n",
    "axes[0, 1].plot(epochs, exact_matches, marker='o', linewidth=2, markersize=4, color='green')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Exact Match Accuracy')\n",
    "axes[0, 1].set_title('Exact Match Accuracy')\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Token Accuracy\n",
    "axes[1, 0].plot(epochs, token_accuracies, marker='o', linewidth=2, markersize=4, color='blue')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Token Accuracy')\n",
    "axes[1, 0].set_title('Token Accuracy')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Per-Attribute F1\n",
    "axes[1, 1].plot(epochs, color_f1s, marker='o', linewidth=2, markersize=4, label='Color F1')\n",
    "axes[1, 1].plot(epochs, shape_f1s, marker='s', linewidth=2, markersize=4, label='Shape F1')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].set_title('Per-Attribute F1 Scores')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "if epochs:\n",
    "    print(f\"\\nFinal Metrics (Epoch {epochs[-1]}):\")\n",
    "    print(f\"  Val Loss: {val_losses[-1]:.4f}\")\n",
    "    print(f\"  Exact Match: {exact_matches[-1]:.4f}\")\n",
    "    print(f\"  Token Accuracy: {token_accuracies[-1]:.4f}\")\n",
    "    print(f\"  Color F1: {color_f1s[-1]:.4f}\")\n",
    "    print(f\"  Shape F1: {shape_f1s[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Predictions\n",
    "\n",
    "Load the best model and visualize predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_checkpoint = checkpoint_dir / 'best_model.pt'\n",
    "\n",
    "if best_checkpoint.exists():\n",
    "    checkpoint = torch.load(best_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"‚úì Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"  Best exact match: {checkpoint['best_exact_match']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Best model checkpoint not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captioner.decode import greedy_decode\n",
    "\n",
    "def visualize_predictions(model, dataset, num_samples=8, use_constraints=True):\n",
    "    \"\"\"Visualize model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get random samples\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    images = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        sample = dataset[idx]\n",
    "        images.append(sample['image'])\n",
    "        ground_truths.append(sample['text'])\n",
    "    \n",
    "    # Stack images\n",
    "    images_tensor = torch.stack(images).to(device)\n",
    "    \n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        _, predictions = greedy_decode(\n",
    "            model=model,\n",
    "            images=images_tensor,\n",
    "            vocab=vocab,\n",
    "            max_length=32,\n",
    "            use_constraints=use_constraints\n",
    "        )\n",
    "    \n",
    "    # Plot\n",
    "    cols = 4\n",
    "    rows = (num_samples + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (img, gt, pred) in enumerate(zip(images, ground_truths, predictions)):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Show image\n",
    "        ax.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "        \n",
    "        # Check if prediction matches\n",
    "        match = gt == pred\n",
    "        color = 'green' if match else 'red'\n",
    "        \n",
    "        # Title with ground truth and prediction\n",
    "        title = f\"GT: {gt}\\nPred: {pred}\"\n",
    "        ax.set_title(title, fontsize=9, color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide extra axes\n",
    "    for ax in axes[num_samples:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    constraint_str = \"WITH\" if use_constraints else \"WITHOUT\"\n",
    "    fig.suptitle(f\"Model Predictions ({constraint_str} FSM Constraints)\", \n",
    "                 fontsize=14, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = sum(1 for gt, pred in zip(ground_truths, predictions) if gt == pred)\n",
    "    accuracy = correct / len(ground_truths)\n",
    "    print(f\"\\nAccuracy on these samples: {correct}/{len(ground_truths)} = {accuracy:.2%}\")\n",
    "\n",
    "# Visualize with constraints\n",
    "print(\"=== Predictions WITH FSM Constraints ===\")\n",
    "visualize_predictions(model, train_dataset, num_samples=8, use_constraints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without constraints\n",
    "print(\"\\n=== Predictions WITHOUT FSM Constraints ===\")\n",
    "visualize_predictions(model, train_dataset, num_samples=8, use_constraints=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Validation Set\n",
    "\n",
    "Evaluate on the full validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captioner.metrics import evaluate_model\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "metrics = evaluate_model(\n",
    "    model=model,\n",
    "    dataloader=val_loader,\n",
    "    vocab=vocab,\n",
    "    device=device,\n",
    "    use_constraints=True,\n",
    "    max_length=32\n",
    ")\n",
    "\n",
    "# Print results\n",
    "metrics.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attention Weights\n",
    "\n",
    "Show what the model is attending to during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, image, vocab, device):\n",
    "    \"\"\"Visualize attention weights during decoding.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode\n",
    "        image_tensor = image.unsqueeze(0).to(device)\n",
    "        grid_tokens, pooled = model.encode(image_tensor)\n",
    "        \n",
    "        # Initialize decoder\n",
    "        hidden = model.init_decoder_state(pooled)\n",
    "        \n",
    "        # Generate tokens and collect attention\n",
    "        input_token = torch.tensor([vocab.bos_id], device=device)\n",
    "        tokens = [vocab.bos_id]\n",
    "        attention_weights = []\n",
    "        \n",
    "        for _ in range(32):\n",
    "            logits, hidden, attn = model.decode_step(\n",
    "                input_token=input_token,\n",
    "                hidden=hidden,\n",
    "                encoder_out=grid_tokens\n",
    "            )\n",
    "            \n",
    "            next_token = logits.argmax(dim=1).item()\n",
    "            tokens.append(next_token)\n",
    "            attention_weights.append(attn.squeeze(0).cpu().numpy())\n",
    "            \n",
    "            if next_token == vocab.eos_id:\n",
    "                break\n",
    "            \n",
    "            input_token = torch.tensor([next_token], device=device)\n",
    "    \n",
    "    # Decode caption\n",
    "    caption = vocab.decode(tokens)\n",
    "    token_strs = [vocab.id_to_token.get(t, f'<{t}>') for t in tokens[1:-1]]  # Skip BOS/EOS\n",
    "    \n",
    "    # Plot attention heatmap\n",
    "    attention_matrix = np.array(attention_weights[:-1])  # Skip EOS\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Show image\n",
    "    axes[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "    axes[0].set_title('Input Image', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show attention heatmap\n",
    "    im = axes[1].imshow(attention_matrix, cmap='viridis', aspect='auto')\n",
    "    axes[1].set_yticks(range(len(token_strs)))\n",
    "    axes[1].set_yticklabels(token_strs, fontsize=10)\n",
    "    axes[1].set_xlabel('Spatial Position (4√ó4 grid, flattened)', fontsize=10)\n",
    "    axes[1].set_ylabel('Generated Token', fontsize=10)\n",
    "    axes[1].set_title('Attention Weights', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=axes[1], label='Attention Weight')\n",
    "    \n",
    "    fig.suptitle(f'Caption: \"{caption}\"', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention for a few samples\n",
    "for i in range(3):\n",
    "    idx = np.random.randint(len(train_dataset))\n",
    "    sample = train_dataset[idx]\n",
    "    visualize_attention(model, sample['image'], vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Find and visualize common failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(model, dataset, num_samples=100):\n",
    "    \"\"\"Analyze prediction errors.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    # Sample predictions\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for idx in tqdm(indices, desc=\"Analyzing errors\"):\n",
    "        sample = dataset[idx]\n",
    "        image = sample['image'].unsqueeze(0).to(device)\n",
    "        gt = sample['text']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, preds = greedy_decode(model, image, vocab, use_constraints=True)\n",
    "        \n",
    "        pred = preds[0]\n",
    "        \n",
    "        if pred != gt:\n",
    "            errors.append({\n",
    "                'image': sample['image'],\n",
    "                'gt': gt,\n",
    "                'pred': pred,\n",
    "                'idx': idx\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nFound {len(errors)} errors out of {num_samples} samples\")\n",
    "    print(f\"Accuracy: {(num_samples - len(errors)) / num_samples:.2%}\")\n",
    "    \n",
    "    # Show some errors\n",
    "    if errors:\n",
    "        num_show = min(8, len(errors))\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, error in enumerate(errors[:num_show]):\n",
    "            ax = axes[i]\n",
    "            ax.imshow(error['image'].permute(1, 2, 0).cpu().numpy())\n",
    "            title = f\"GT: {error['gt']}\\nPred: {error['pred']}\"\n",
    "            ax.set_title(title, fontsize=9, color='red')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Error Cases', fontsize=14, fontweight='bold', y=1.00)\n",
    "        plt.show()\n",
    "    \n",
    "    return errors\n",
    "\n",
    "errors = analyze_errors(model, train_dataset, num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Compositional Generalization\n",
    "\n",
    "Test on holdout splits to evaluate compositional understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate holdout test sets\nprint(\"Generating compositional holdout test sets...\")\n\n# Color-shape holdout\n!python -m data.gen \\\n    --out_dir ./data/scenes_color_holdout \\\n    --n 500 \\\n    --split_strategy color_shape \\\n    --seed 42\n\n# Relation holdout\n!python -m data.gen \\\n    --out_dir ./data/scenes_relation_holdout \\\n    --n 500 \\\n    --split_strategy relation \\\n    --seed 42\n\nprint(\"\\n‚úÖ Holdout datasets generated!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on holdout splits\n",
    "from data.dataset import SceneDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate_on_split(model, data_dir, split_name):\n",
    "    \"\"\"Evaluate on a specific data split.\"\"\"\n",
    "    dataset = SceneDataset(data_dir, 'test', vocab)\n",
    "    loader = DataLoader(dataset, batch_size=64, num_workers=2)\n",
    "    \n",
    "    metrics = evaluate_model(model, loader, vocab, device, use_constraints=True)\n",
    "    results = metrics.compute()\n",
    "    \n",
    "    print(f\"\\n=== {split_name} ===\")\n",
    "    print(f\"  Exact Match: {results['exact_match']:.4f}\")\n",
    "    print(f\"  Token Accuracy: {results['token_accuracy']:.4f}\")\n",
    "    print(f\"  Color F1: {results['color_f1']:.4f}\")\n",
    "    print(f\"  Shape F1: {results['shape_f1']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate on different splits\n",
    "results_iid = evaluate_on_split(model, './data/scenes', 'IID (Random Split)')\n",
    "results_color = evaluate_on_split(model, './data/scenes_color_holdout', 'Color-Shape Holdout')\n",
    "results_relation = evaluate_on_split(model, './data/scenes_relation_holdout', 'Relation Holdout')\n",
    "\n",
    "# Compare results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "splits = ['IID', 'Color Holdout', 'Relation Holdout']\n",
    "exact_matches = [\n",
    "    results_iid['exact_match'],\n",
    "    results_color['exact_match'],\n",
    "    results_relation['exact_match']\n",
    "]\n",
    "token_accs = [\n",
    "    results_iid['token_accuracy'],\n",
    "    results_color['token_accuracy'],\n",
    "    results_relation['token_accuracy']\n",
    "]\n",
    "\n",
    "x = np.arange(len(splits))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, exact_matches, width, label='Exact Match', alpha=0.8)\n",
    "ax.bar(x + width/2, token_accs, width, label='Token Accuracy', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Compositional Generalization: Performance on Different Splits')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(splits)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model\n",
    "\n",
    "Save the trained model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Google Drive\n",
    "import shutil\n",
    "\n",
    "# Copy best model to Drive\n",
    "drive_checkpoint_dir = '/content/drive/MyDrive/learning_to_see_checkpoints'\n",
    "Path(drive_checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "best_model_path = checkpoint_dir / 'best_model.pt'\n",
    "if best_model_path.exists():\n",
    "    shutil.copy(best_model_path, drive_checkpoint_dir)\n",
    "    print(f\"‚úì Saved best model to: {drive_checkpoint_dir}/best_model.pt\")\n",
    "\n",
    "# Save final metrics\n",
    "final_metrics = {\n",
    "    'iid': results_iid,\n",
    "    'color_holdout': results_color,\n",
    "    'relation_holdout': results_relation\n",
    "}\n",
    "\n",
    "with open(Path(drive_checkpoint_dir) / 'final_metrics.json', 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "\n",
    "print(\"‚úì Saved final metrics to Drive\")\n",
    "print(\"\\nüéâ Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}