{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9e9db7",
   "metadata": {},
   "source": "# Text-to-Image Distillation Training (C1)\n\nTrain a neural network to generate 64\u00d764 images from text captions.\n\n**Architecture:** 4-layer Transformer encoder + FiLM-conditioned CNN decoder  \n**Parameters:** ~11M total (2.4M encoder + 8.5M decoder)  \n**Training time:** ~6-8 hours for 100k steps on GPU\n\n**Note:** This notebook uses Google Drive for persistent storage."
  },
  {
   "cell_type": "markdown",
   "id": "69e035c1",
   "metadata": {},
   "source": "## 1. Setup and Mount Drive"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df909820",
   "metadata": {},
   "outputs": [],
   "source": "# Mount Google Drive for persistent storage\nfrom google.colab import drive\ndrive.mount('/content/drive')\nprint(\"\u2713 Drive mounted\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb085406",
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU\nimport torch\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111662d",
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo to Drive (persistent) or local (temporary)\nimport os\n\n# Use Drive for persistence (recommended)\nWORK_DIR = '/content/drive/MyDrive/learning_to_see'\n\n# Or use local for faster I/O (but data lost on disconnect)\n# WORK_DIR = '/content/learning_to_see'\n\nif not os.path.exists(WORK_DIR):\n    !git clone https://github.com/jtooates/learning_to_see.git {WORK_DIR}\n    print(f\"\u2713 Cloned to {WORK_DIR}\")\nelse:\n    print(f\"\u2713 Found existing repo at {WORK_DIR}\")\n\n%cd {WORK_DIR}\n!git pull  # Get latest changes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0298ada",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q pillow numpy regex tqdm pyyaml matplotlib ipywidgets pytest\n\n# Add to Python path\nimport sys\nsys.path.insert(0, WORK_DIR)\nprint(\"\u2713 Dependencies installed\")"
  },
  {
   "cell_type": "markdown",
   "id": "a3686c59",
   "metadata": {},
   "source": "## 2. Generate Data\n\n**Note:** Data will be saved to your Google Drive (persistent) or local (temporary based on WORK_DIR choice).  \nFirst run takes ~10-20 minutes for 2000 scenes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f991ad",
   "metadata": {},
   "outputs": [],
   "source": "# Check if data already exists\nimport os\ndata_path = 'data/scenes'\n\nif os.path.exists(data_path) and os.path.exists(f'{data_path}/manifest.json'):\n    print(f\"\u2713 Found existing data at {data_path}\")\n    print(\"Skipping generation. Delete the folder to regenerate.\")\nelse:\n    print(\"Generating new dataset...\")\n    !python -m data.gen --out_dir {data_path} --n 2000 --split_strategy random --seed 42\n    print(\"\u2713 Data generation complete\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f53b5",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize samples\n!python visualize_samples.py --data_dir data/scenes --n 16 --save_path samples.png\n\nimport os\nfrom IPython.display import Image, display\nif os.path.exists('samples.png'):\n    display(Image('samples.png', width=800))\nelse:\n    print(\"Note: Preview not available\")"
  },
  {
   "cell_type": "markdown",
   "id": "e66be154",
   "metadata": {},
   "source": "## 3. Run Tests (Optional but Recommended)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3963f52",
   "metadata": {},
   "outputs": [],
   "source": "# Verify all components work\n!python -m pytest distill_c1/tests_distill.py -v --tb=short\n\nprint(\"\n\u2713 All tests passed!\")"
  },
  {
   "cell_type": "markdown",
   "id": "f90b4079",
   "metadata": {},
   "source": "## 4. Training Configuration\n\n**Adjust batch size based on your GPU:**\n- T4 (free Colab): batch=64 or batch=96\n- A100 (Colab Pro): batch=192\n- V100: batch=128\n\n**Training time estimates:**\n- 30k steps: ~2.5 hours\n- 50k steps: ~4 hours  \n- 100k steps: ~8 hours"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abac24f",
   "metadata": {},
   "outputs": [],
   "source": "# Configure training\nconfig = {\n    'data_dir': 'data/scenes',\n    'save_dir': 'runs/distill_c1',  # Saved in your working directory\n    'steps': 30000,      # Reduce for faster (full: 100000)\n    'batch': 96,         # Adjust for GPU (T4: 64-96, A100: 192)\n    'eval_every': 2000,\n    'lr': 3e-4,\n    'seed': 1337,\n}\n\nprint(\"Training Configuration:\")\nprint(\"=\" * 40)\nfor k, v in config.items():\n    print(f\"{k:15s}: {v}\")\nprint(\"=\" * 40)\nprint(f\"\nEstimated time: ~{config['steps'] / 12000:.1f} hours\")"
  },
  {
   "cell_type": "markdown",
   "id": "e3e89147",
   "metadata": {},
   "source": "## 5. Quick Sanity Check (Optional)\n\nRun a very short training (500 steps, ~5 minutes) to verify everything works."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c21698",
   "metadata": {},
   "outputs": [],
   "source": "# Quick sanity check\n!python -m distill_c1.train_distill   --data_dir data/scenes   --save_dir runs/sanity_check   --steps 500   --batch 64   --eval_every 250   --seed 42\n\nprint(\"\n\u2713 Sanity check complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "9b92079d",
   "metadata": {},
   "source": "## 6. Full Training\n\n**Important:** This will take several hours. The notebook must stay connected.  \nProgress is saved every eval_every steps, so you can resume if disconnected."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992f501",
   "metadata": {},
   "outputs": [],
   "source": "# Check for existing checkpoint to resume\nimport os\nresume_ckpt = f\"{config['save_dir']}/last.pt\"\nresume_flag = f\"--resume {resume_ckpt}\" if os.path.exists(resume_ckpt) else \"\"\n\nif resume_flag:\n    print(f\"\u2713 Found checkpoint, will resume from {resume_ckpt}\")\nelse:\n    print(\"Starting fresh training\")\n\n# Train model\n!python -m distill_c1.train_distill   --data_dir {config['data_dir']}   --save_dir {config['save_dir']}   --steps {config['steps']}   --batch {config['batch']}   --eval_every {config['eval_every']}   --lr {config['lr']}   --seed {config['seed']}   --use_amp   {resume_flag}\n\nprint(\"\n\u2713 Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "421f824b",
   "metadata": {},
   "source": "## 7. Visualize Training Progress"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd28ea",
   "metadata": {},
   "outputs": [],
   "source": "# Plot training metrics\nimport json\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nlog_path = Path(config['save_dir']) / 'log.json'\n\nif not log_path.exists():\n    print(\"No training log found. Train the model first (Section 6).\")\nelse:\n    with open(log_path) as f:\n        log = json.load(f)\n    \n    steps = [e['step'] for e in log]\n    psnr = [e['metrics']['psnr'] for e in log]\n    ssim = [e['metrics']['ssim'] for e in log]\n    loss = [e['metrics']['total'] for e in log]\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    # PSNR\n    axes[0].plot(steps, psnr, linewidth=2)\n    axes[0].axhline(24, color='r', linestyle='--', label='Target (24 dB)', alpha=0.7)\n    axes[0].set_xlabel('Step', fontsize=12)\n    axes[0].set_ylabel('PSNR (dB)', fontsize=12)\n    axes[0].set_title('Peak Signal-to-Noise Ratio', fontsize=13)\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # SSIM\n    axes[1].plot(steps, ssim, linewidth=2, color='orange')\n    axes[1].axhline(0.92, color='r', linestyle='--', label='Target (0.92)', alpha=0.7)\n    axes[1].set_xlabel('Step', fontsize=12)\n    axes[1].set_ylabel('SSIM', fontsize=12)\n    axes[1].set_title('Structural Similarity', fontsize=13)\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    # Loss\n    axes[2].plot(steps, loss, linewidth=2, color='green')\n    axes[2].set_xlabel('Step', fontsize=12)\n    axes[2].set_ylabel('Loss', fontsize=12)\n    axes[2].set_title('Training Loss', fontsize=13)\n    axes[2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\nFinal Metrics (Step {steps[-1]}):\")\n    print(f\"  PSNR: {psnr[-1]:.2f} dB\")\n    print(f\"  SSIM: {ssim[-1]:.4f}\")\n    print(f\"  Loss: {loss[-1]:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e36708",
   "metadata": {},
   "outputs": [],
   "source": "# View generated samples during training\nfrom IPython.display import Image, display\nfrom pathlib import Path\nimport glob\n\nsample_dir = Path(config['save_dir']) / 'samples'\nsample_files = sorted(glob.glob(str(sample_dir / 'step_*.png')))\n\nif not sample_files:\n    print(\"No sample images found yet.\")\nelse:\n    print(f\"Found {len(sample_files)} sample grids\n\")\n    print(\"Showing samples at different stages:\n\")\n    \n    # Show first, middle, and last\n    indices = [0, len(sample_files)//2, -1]\n    for idx in indices:\n        file = sample_files[idx]\n        step = Path(file).stem.split('_')[1]\n        print(f\"Step {step}:\")\n        display(Image(file, width=800))\n        print()"
  },
  {
   "cell_type": "markdown",
   "id": "c1d85a60",
   "metadata": {},
   "source": "## 8. Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b78ca",
   "metadata": {},
   "outputs": [],
   "source": "# Run comprehensive evaluation\n!python -m distill_c1.eval_distill   --data_dir {config['data_dir']}   --ckpt {config['save_dir']}/ema_best.pt   --report {config['save_dir']}/report.json   --save_images {config['save_dir']}/eval_images   --counterfactual\n\nprint(\"\n\u2713 Evaluation complete\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a059f",
   "metadata": {},
   "outputs": [],
   "source": "# Display evaluation report\nfrom pathlib import Path\nimport json\n\nreport_path = Path(config['save_dir']) / 'report.json'\n\nif report_path.exists():\n    with open(report_path) as f:\n        report = json.load(f)\n    \n    print(\"Evaluation Report\")\n    print(\"=\" * 50)\n    print(f\"Split: {report['split']}\")\n    print(f\"Samples: {report['num_samples']}\")\n    print(f\"\nMetrics:\")\n    print(f\"  PSNR: {report['metrics']['psnr']:.2f} dB\")\n    print(f\"  SSIM: {report['metrics']['ssim']:.4f}\")\n    \n    psnr_pass = report['metrics']['psnr'] >= 24.0\n    ssim_pass = report['metrics']['ssim'] >= 0.92\n    \n    print(f\"\nAcceptance Criteria:\")\n    print(f\"  PSNR >= 24 dB: {'\u2713 PASS' if psnr_pass else '\u2717 FAIL'}\")\n    print(f\"  SSIM >= 0.92:  {'\u2713 PASS' if ssim_pass else '\u2717 FAIL'}\")\n    \n    if 'counterfactual' in report and report['counterfactual']:\n        print(f\"\nCounterfactual Sensitivity:\")\n        for edit_type, results in report['counterfactual'].items():\n            print(f\"  {edit_type}: \u0394L2 = {results['avg_delta_l2']:.6f}\")\n    \n    print(\"=\" * 50)\nelse:\n    print(\"No report found. Run evaluation first (Section 8).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c6dee",
   "metadata": {},
   "outputs": [],
   "source": "# Show evaluation visualizations\nfrom IPython.display import Image, display\nfrom pathlib import Path\n\neval_dir = Path(config['save_dir']) / 'eval_images'\n\nif (eval_dir / 'grid.png').exists():\n    print(\"Teacher vs Student vs Difference:\n\")\n    display(Image(str(eval_dir / 'grid.png'), width=900))\n    \n    print(\"\n\nSide-by-Side Comparison:\n\")\n    display(Image(str(eval_dir / 'comparison.png'), width=600))\nelse:\n    print(\"No evaluation images found. Run evaluation first (Section 8).\")"
  },
  {
   "cell_type": "markdown",
   "id": "c3f28319",
   "metadata": {},
   "source": "## 9. Interactive Generation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5fad4",
   "metadata": {},
   "outputs": [],
   "source": "# Load trained model\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom distill_c1.text_encoder import build_text_encoder\nfrom distill_c1.decoder import build_decoder\nfrom dsl.tokens import Vocab\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load vocabulary\nvocab = Vocab()\n\n# Build models\ntext_encoder = build_text_encoder(vocab_size=len(vocab), pad_id=vocab.pad_id)\ndecoder = build_decoder()\n\n# Load checkpoint\nckpt_path = Path(config['save_dir']) / 'ema_best.pt'\nif not ckpt_path.exists():\n    print(f\"Checkpoint not found: {ckpt_path}\")\n    print(\"Train the model first (Section 6).\")\nelse:\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    text_encoder.load_state_dict(checkpoint['text_encoder'])\n    decoder.load_state_dict(checkpoint['decoder'])\n    \n    text_encoder.to(device).eval()\n    decoder.to(device).eval()\n    \n    print(f\"\u2713 Model loaded successfully\")\n    print(f\"Device: {device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf211b",
   "metadata": {},
   "outputs": [],
   "source": "# Generation functions\ndef generate(text):\n    \"\"\"Generate image from text.\"\"\"\n    tokens = vocab.encode(text, add_special_tokens=True)\n    token_ids = torch.tensor([tokens], dtype=torch.long, device=device)\n    \n    with torch.no_grad():\n        e = text_encoder(token_ids, vocab.pad_id)\n        img = decoder(e)\n    \n    # Convert to numpy\n    img = img.squeeze(0).cpu()\n    img = (img + 1.0) / 2.0  # [-1, 1] -> [0, 1]\n    img = img.permute(1, 2, 0).numpy()\n    return np.clip(img, 0, 1)\n\ndef show(text):\n    \"\"\"Generate and display.\"\"\"\n    plt.figure(figsize=(5, 5))\n    plt.imshow(generate(text))\n    plt.title(text, fontsize=12, pad=10)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nprint(\"\u2713 Generation functions ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5562d",
   "metadata": {},
   "outputs": [],
   "source": "# Example generations\nprompts = [\n    \"There is one red ball.\",\n    \"There are two green cubes.\",\n    \"There are three yellow blocks.\",\n    \"The blue ball is left of the red cube.\",\n    \"The green block is on the yellow ball.\",\n    \"The red cube is in front of the blue block.\",\n]\n\nprint(\"Generating images from prompts...\n\")\n\nfor prompt in prompts:\n    print(f\"Prompt: {prompt}\")\n    show(prompt)\n    print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771365c0",
   "metadata": {},
   "outputs": [],
   "source": "# Interactive generation with custom prompts\nfrom ipywidgets import interact, Text\n\ndef generate_interactive(prompt):\n    if prompt.strip():\n        show(prompt)\n    else:\n        print(\"Enter a prompt in DSL format:\")\n        print('  \"There is one red ball.\"')\n        print('  \"There are two green cubes.\"')\n        print('  \"The blue ball is left of the red cube.\"')\n\ninteract(generate_interactive, \n         prompt=Text(value=\"There is one red ball.\", \n                    description=\"Prompt:\",\n                    style={'description_width': 'initial'}))"
  },
  {
   "cell_type": "markdown",
   "id": "f6e41fb5",
   "metadata": {},
   "source": "## 10. Counterfactual Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d1b65",
   "metadata": {},
   "outputs": [],
   "source": "# Compare how text changes affect images\ndef compare(prompt1, prompt2):\n    \"\"\"Compare two prompts.\"\"\"\n    img1 = generate(prompt1)\n    img2 = generate(prompt2)\n    diff = np.abs(img1 - img2)\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    axes[0].imshow(img1)\n    axes[0].set_title(prompt1, fontsize=10)\n    axes[0].axis('off')\n    \n    axes[1].imshow(img2)\n    axes[1].set_title(prompt2, fontsize=10)\n    axes[1].axis('off')\n    \n    axes[2].imshow(diff, cmap='hot')\n    axes[2].set_title('Difference (hotter = more change)', fontsize=10)\n    axes[2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    l2_diff = np.mean(diff ** 2)\n    print(f\"L2 Difference: {l2_diff:.6f}\n\")\n\n# Color change\nprint(\"Color Change:\")\ncompare(\"There is one red ball.\", \"There is one blue ball.\")\n\n# Shape change\nprint(\"Shape Change:\")\ncompare(\"There is one red ball.\", \"There is one red cube.\")\n\n# Number change\nprint(\"Number Change:\")\ncompare(\"There are two green cubes.\", \"There are three green cubes.\")\n\n# Relation change\nprint(\"Relation Change:\")\ncompare(\"The blue ball is left of the red cube.\", \n        \"The blue ball is right of the red cube.\")"
  },
  {
   "cell_type": "markdown",
   "id": "81f17096",
   "metadata": {},
   "source": "## Summary\n\n\u2705 **Completed:**\n1. Data generation with DSL and renderer\n2. Model training with FiLM-conditioned decoder\n3. Evaluation with PSNR/SSIM metrics\n4. Interactive image generation\n5. Counterfactual sensitivity analysis\n\n**Key Results:**\n- Total parameters: ~11M (2.4M encoder + 8.5M decoder)\n- Target metrics: PSNR \u2265 24 dB, SSIM \u2265 0.92\n- Training time: ~6-8 hours for 100k steps\n\n**Next Steps:**\n- Phase C2: Fine-tune with caption loss only\n- Phase C3: Add adversarial losses\n- Experiment with different compositions\n\n**Data Location:**\nAll outputs are saved in your working directory (Drive or local)."
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}