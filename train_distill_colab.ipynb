{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bec8074",
   "metadata": {},
   "source": [
    "# Text-to-Image Distillation Training (C1)\n",
    "\n",
    "Train a neural network to generate 64×64 images from text captions.\n",
    "\n",
    "**Architecture:** 4-layer Transformer encoder + FiLM-conditioned CNN decoder  \n",
    "**Parameters:** ~11M total (2.4M encoder + 8.5M decoder)  \n",
    "**Training time:** ~6-8 hours for 100k steps on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051634d",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7697ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1968780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo\n",
    "import os\n",
    "if not os.path.exists('learning_to_see'):\n",
    "    !git clone https://github.com/jtooates/learning_to_see.git\n",
    "%cd learning_to_see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pillow numpy regex tqdm pyyaml matplotlib ipywidgets pytest\n",
    "import sys\n",
    "sys.path.insert(0, '/content/learning_to_see')\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf8e11",
   "metadata": {},
   "source": [
    "## 2. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4eb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scenes (adjust --n for dataset size)\n",
    "!python -m data.gen --out_dir data/scenes --n 2000 --split_strategy random --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "!python visualize_samples.py --data_dir data/scenes --n 16 --save_path samples.png\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "if os.path.exists('samples.png'):\n",
    "    display(Image('samples.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb663a9",
   "metadata": {},
   "source": [
    "## 3. Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest distill_c1/tests_distill.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7123378",
   "metadata": {},
   "source": [
    "## 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training\n",
    "config = {\n",
    "    'data_dir': 'data/scenes',\n",
    "    'save_dir': 'runs/distill_c1',\n",
    "    'steps': 30000,      # Reduce for faster (full: 100000)\n",
    "    'batch': 96,         # Adjust for GPU (T4: 64, A100: 192)\n",
    "    'eval_every': 2000,\n",
    "    'lr': 3e-4,\n",
    "    'seed': 1337,\n",
    "}\n",
    "\n",
    "for k, v in config.items():\n",
    "    print(f\"{k:15s}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9bb89c",
   "metadata": {},
   "source": [
    "## 5. Quick Test (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed96b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check (~5 min)\n",
    "!python -m distill_c1.train_distill   --data_dir data/scenes   --save_dir runs/test   --steps 500   --batch 64   --eval_every 250   --seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09326ef0",
   "metadata": {},
   "source": [
    "## 6. Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de74de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "!python -m distill_c1.train_distill   --data_dir {config['data_dir']}   --save_dir {config['save_dir']}   --steps {config['steps']}   --batch {config['batch']}   --eval_every {config['eval_every']}   --lr {config['lr']}   --seed {config['seed']}   --use_amp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc198e",
   "metadata": {},
   "source": [
    "## 7. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "import json, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path(config['save_dir']) / 'log.json') as f:\n",
    "    log = json.load(f)\n",
    "\n",
    "steps = [e['step'] for e in log]\n",
    "psnr = [e['metrics']['psnr'] for e in log]\n",
    "ssim = [e['metrics']['ssim'] for e in log]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(steps, psnr)\n",
    "ax1.axhline(24, color='r', linestyle='--', label='Target')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('PSNR (dB)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(steps, ssim, color='orange')\n",
    "ax2.axhline(0.92, color='r', linestyle='--', label='Target')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('SSIM')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final - PSNR: {psnr[-1]:.2f} dB, SSIM: {ssim[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7e52e",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6527e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "!python -m distill_c1.eval_distill   --data_dir {config['data_dir']}   --ckpt {config['save_dir']}/ema_best.pt   --report {config['save_dir']}/report.json   --save_images {config['save_dir']}/eval_images   --counterfactual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show report\n",
    "with open(Path(config['save_dir']) / 'report.json') as f:\n",
    "    report = json.load(f)\n",
    "\n",
    "print(f\"PSNR: {report['metrics']['psnr']:.2f} dB\")\n",
    "print(f\"SSIM: {report['metrics']['ssim']:.4f}\")\n",
    "print(f\"PSNR >= 24: {'✓' if report['metrics']['psnr'] >= 24 else '✗'}\")\n",
    "print(f\"SSIM >= 0.92: {'✓' if report['metrics']['ssim'] >= 0.92 else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31141810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show eval images\n",
    "from IPython.display import Image, display\n",
    "eval_dir = Path(config['save_dir']) / 'eval_images'\n",
    "display(Image(str(eval_dir / 'grid.png'), width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824757c",
   "metadata": {},
   "source": [
    "## 9. Interactive Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f97c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "import torch, numpy as np\n",
    "from distill_c1.text_encoder import build_text_encoder\n",
    "from distill_c1.decoder import build_decoder\n",
    "from dsl.tokens import Vocab\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab = Vocab()\n",
    "\n",
    "text_encoder = build_text_encoder(vocab_size=len(vocab), pad_id=vocab.pad_id)\n",
    "decoder = build_decoder()\n",
    "\n",
    "ckpt = torch.load(Path(config['save_dir']) / 'ema_best.pt', map_location=device)\n",
    "text_encoder.load_state_dict(ckpt['text_encoder'])\n",
    "decoder.load_state_dict(ckpt['decoder'])\n",
    "\n",
    "text_encoder.to(device).eval()\n",
    "decoder.to(device).eval()\n",
    "\n",
    "print(f\"✓ Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b73bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate function\n",
    "def generate(text):\n",
    "    tokens = vocab.encode(text, add_special_tokens=True)\n",
    "    token_ids = torch.tensor([tokens], device=device)\n",
    "    with torch.no_grad():\n",
    "        e = text_encoder(token_ids, vocab.pad_id)\n",
    "        img = decoder(e)\n",
    "    img = ((img[0].cpu() + 1) / 2).permute(1, 2, 0).numpy()\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "def show(text):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(generate(text))\n",
    "    plt.title(text, fontsize=12)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Ready to generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea229c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example generations\n",
    "prompts = [\n",
    "    \"There is one red ball.\",\n",
    "    \"There are two green cubes.\",\n",
    "    \"The blue ball is left of the red cube.\",\n",
    "    \"The green block is on the yellow ball.\",\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3144ba0",
   "metadata": {},
   "source": [
    "## 10. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip\n",
    "import shutil\n",
    "shutil.make_archive('results', 'zip', config['save_dir'])\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('results.zip')\n",
    "    print(\"✓ Download started\")\n",
    "except:\n",
    "    print(\"✓ results.zip created\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
