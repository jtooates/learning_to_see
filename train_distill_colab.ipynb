{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9e9db7",
   "metadata": {},
   "source": [
    "# Text-to-Image Distillation Training (C1)\n",
    "\n",
    "Train a neural network to generate 64×64 images from text captions.\n",
    "\n",
    "**Architecture:** 4-layer Transformer encoder + FiLM-conditioned CNN decoder  \n",
    "**Parameters:** ~11M total (2.4M encoder + 8.5M decoder)  \n",
    "**Training time:** ~6-8 hours for 100k steps on GPU\n",
    "\n",
    "**Note:** This notebook uses Google Drive for persistent storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e035c1",
   "metadata": {},
   "source": [
    "## 1. Setup and Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df909820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"✓ Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb085406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo to Drive (persistent) or local (temporary)\n",
    "import os\n",
    "\n",
    "# Use Drive for persistence (recommended)\n",
    "WORK_DIR = '/content/drive/MyDrive/learning_to_see'\n",
    "\n",
    "# Or use local for faster I/O (but data lost on disconnect)\n",
    "# WORK_DIR = '/content/learning_to_see'\n",
    "\n",
    "if not os.path.exists(WORK_DIR):\n",
    "    !git clone https://github.com/jtooates/learning_to_see.git {WORK_DIR}\n",
    "    print(f\"✓ Cloned to {WORK_DIR}\")\n",
    "else:\n",
    "    print(f\"✓ Found existing repo at {WORK_DIR}\")\n",
    "\n",
    "%cd {WORK_DIR}\n",
    "!git pull  # Get latest changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0298ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pillow numpy regex tqdm pyyaml matplotlib ipywidgets pytest\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.insert(0, WORK_DIR)\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3686c59",
   "metadata": {},
   "source": [
    "## 2. Generate Data\n",
    "\n",
    "**Note:** Data will be saved to your Google Drive (persistent) or local (temporary based on WORK_DIR choice).  \n",
    "First run takes ~10-20 minutes for 2000 scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f991ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data already exists\n",
    "import os\n",
    "data_path = 'data/scenes'\n",
    "\n",
    "if os.path.exists(data_path) and os.path.exists(f'{data_path}/manifest.json'):\n",
    "    print(f\"✓ Found existing data at {data_path}\")\n",
    "    print(\"Skipping generation. Delete the folder to regenerate.\")\n",
    "else:\n",
    "    print(\"Generating new dataset...\")\n",
    "    !python -m data.gen --out_dir {data_path} --n 2000 --split_strategy random --seed 42\n",
    "    print(\"✓ Data generation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "!python visualize_samples.py --data_dir data/scenes --n 16 --save_path samples.png\n",
    "\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "if os.path.exists('samples.png'):\n",
    "    display(Image('samples.png', width=800))\n",
    "else:\n",
    "    print(\"Note: Preview not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66be154",
   "metadata": {},
   "source": [
    "## 3. Run Tests (Optional but Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3963f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all components work\n",
    "!python -m pytest distill_c1/tests_distill.py -v --tb=short\n",
    "\n",
    "print(\"\n",
    "✓ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90b4079",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "**Adjust batch size based on your GPU:**\n",
    "- T4 (free Colab): batch=64 or batch=96\n",
    "- A100 (Colab Pro): batch=192\n",
    "- V100: batch=128\n",
    "\n",
    "**Training time estimates:**\n",
    "- 30k steps: ~2.5 hours\n",
    "- 50k steps: ~4 hours  \n",
    "- 100k steps: ~8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abac24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training\n",
    "config = {\n",
    "    'data_dir': 'data/scenes',\n",
    "    'save_dir': 'runs/distill_c1',  # Saved in your working directory\n",
    "    'steps': 30000,      # Reduce for faster (full: 100000)\n",
    "    'batch': 96,         # Adjust for GPU (T4: 64-96, A100: 192)\n",
    "    'eval_every': 2000,\n",
    "    'lr': 3e-4,\n",
    "    'seed': 1337,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 40)\n",
    "for k, v in config.items():\n",
    "    print(f\"{k:15s}: {v}\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"\n",
    "Estimated time: ~{config['steps'] / 12000:.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e89147",
   "metadata": {},
   "source": [
    "## 5. Quick Sanity Check (Optional)\n",
    "\n",
    "Run a very short training (500 steps, ~5 minutes) to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c21698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check\n",
    "!python -m distill_c1.train_distill   --data_dir data/scenes   --save_dir runs/sanity_check   --steps 500   --batch 64   --eval_every 250   --seed 42\n",
    "\n",
    "print(\"\n",
    "✓ Sanity check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92079d",
   "metadata": {},
   "source": [
    "## 6. Full Training\n",
    "\n",
    "**Important:** This will take several hours. The notebook must stay connected.  \n",
    "Progress is saved every eval_every steps, so you can resume if disconnected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoint to resume\n",
    "import os\n",
    "resume_ckpt = f\"{config['save_dir']}/last.pt\"\n",
    "resume_flag = f\"--resume {resume_ckpt}\" if os.path.exists(resume_ckpt) else \"\"\n",
    "\n",
    "if resume_flag:\n",
    "    print(f\"✓ Found checkpoint, will resume from {resume_ckpt}\")\n",
    "else:\n",
    "    print(\"Starting fresh training\")\n",
    "\n",
    "# Train model\n",
    "!python -m distill_c1.train_distill   --data_dir {config['data_dir']}   --save_dir {config['save_dir']}   --steps {config['steps']}   --batch {config['batch']}   --eval_every {config['eval_every']}   --lr {config['lr']}   --seed {config['seed']}   --use_amp   {resume_flag}\n",
    "\n",
    "print(\"\n",
    "✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f824b",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "log_path = Path(config['save_dir']) / 'log.json'\n",
    "\n",
    "if not log_path.exists():\n",
    "    print(\"No training log found. Train the model first (Section 6).\")\n",
    "else:\n",
    "    with open(log_path) as f:\n",
    "        log = json.load(f)\n",
    "    \n",
    "    steps = [e['step'] for e in log]\n",
    "    psnr = [e['metrics']['psnr'] for e in log]\n",
    "    ssim = [e['metrics']['ssim'] for e in log]\n",
    "    loss = [e['metrics']['total'] for e in log]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # PSNR\n",
    "    axes[0].plot(steps, psnr, linewidth=2)\n",
    "    axes[0].axhline(24, color='r', linestyle='--', label='Target (24 dB)', alpha=0.7)\n",
    "    axes[0].set_xlabel('Step', fontsize=12)\n",
    "    axes[0].set_ylabel('PSNR (dB)', fontsize=12)\n",
    "    axes[0].set_title('Peak Signal-to-Noise Ratio', fontsize=13)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SSIM\n",
    "    axes[1].plot(steps, ssim, linewidth=2, color='orange')\n",
    "    axes[1].axhline(0.92, color='r', linestyle='--', label='Target (0.92)', alpha=0.7)\n",
    "    axes[1].set_xlabel('Step', fontsize=12)\n",
    "    axes[1].set_ylabel('SSIM', fontsize=12)\n",
    "    axes[1].set_title('Structural Similarity', fontsize=13)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    axes[2].plot(steps, loss, linewidth=2, color='green')\n",
    "    axes[2].set_xlabel('Step', fontsize=12)\n",
    "    axes[2].set_ylabel('Loss', fontsize=12)\n",
    "    axes[2].set_title('Training Loss', fontsize=13)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\n",
    "Final Metrics (Step {steps[-1]}):\")\n",
    "    print(f\"  PSNR: {psnr[-1]:.2f} dB\")\n",
    "    print(f\"  SSIM: {ssim[-1]:.4f}\")\n",
    "    print(f\"  Loss: {loss[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e36708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View generated samples during training\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "sample_dir = Path(config['save_dir']) / 'samples'\n",
    "sample_files = sorted(glob.glob(str(sample_dir / 'step_*.png')))\n",
    "\n",
    "if not sample_files:\n",
    "    print(\"No sample images found yet.\")\n",
    "else:\n",
    "    print(f\"Found {len(sample_files)} sample grids\n",
    "\")\n",
    "    print(\"Showing samples at different stages:\n",
    "\")\n",
    "    \n",
    "    # Show first, middle, and last\n",
    "    indices = [0, len(sample_files)//2, -1]\n",
    "    for idx in indices:\n",
    "        file = sample_files[idx]\n",
    "        step = Path(file).stem.split('_')[1]\n",
    "        print(f\"Step {step}:\")\n",
    "        display(Image(file, width=800))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d85a60",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "!python -m distill_c1.eval_distill   --data_dir {config['data_dir']}   --ckpt {config['save_dir']}/ema_best.pt   --report {config['save_dir']}/report.json   --save_images {config['save_dir']}/eval_images   --counterfactual\n",
    "\n",
    "print(\"\n",
    "✓ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation report\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "report_path = Path(config['save_dir']) / 'report.json'\n",
    "\n",
    "if report_path.exists():\n",
    "    with open(report_path) as f:\n",
    "        report = json.load(f)\n",
    "    \n",
    "    print(\"Evaluation Report\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Split: {report['split']}\")\n",
    "    print(f\"Samples: {report['num_samples']}\")\n",
    "    print(f\"\n",
    "Metrics:\")\n",
    "    print(f\"  PSNR: {report['metrics']['psnr']:.2f} dB\")\n",
    "    print(f\"  SSIM: {report['metrics']['ssim']:.4f}\")\n",
    "    \n",
    "    psnr_pass = report['metrics']['psnr'] >= 24.0\n",
    "    ssim_pass = report['metrics']['ssim'] >= 0.92\n",
    "    \n",
    "    print(f\"\n",
    "Acceptance Criteria:\")\n",
    "    print(f\"  PSNR >= 24 dB: {'✓ PASS' if psnr_pass else '✗ FAIL'}\")\n",
    "    print(f\"  SSIM >= 0.92:  {'✓ PASS' if ssim_pass else '✗ FAIL'}\")\n",
    "    \n",
    "    if 'counterfactual' in report and report['counterfactual']:\n",
    "        print(f\"\n",
    "Counterfactual Sensitivity:\")\n",
    "        for edit_type, results in report['counterfactual'].items():\n",
    "            print(f\"  {edit_type}: ΔL2 = {results['avg_delta_l2']:.6f}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    print(\"No report found. Run evaluation first (Section 8).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show evaluation visualizations\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "eval_dir = Path(config['save_dir']) / 'eval_images'\n",
    "\n",
    "if (eval_dir / 'grid.png').exists():\n",
    "    print(\"Teacher vs Student vs Difference:\n",
    "\")\n",
    "    display(Image(str(eval_dir / 'grid.png'), width=900))\n",
    "    \n",
    "    print(\"\n",
    "\n",
    "Side-by-Side Comparison:\n",
    "\")\n",
    "    display(Image(str(eval_dir / 'comparison.png'), width=600))\n",
    "else:\n",
    "    print(\"No evaluation images found. Run evaluation first (Section 8).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f28319",
   "metadata": {},
   "source": [
    "## 9. Interactive Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from distill_c1.text_encoder import build_text_encoder\n",
    "from distill_c1.decoder import build_decoder\n",
    "from dsl.tokens import Vocab\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load vocabulary\n",
    "vocab = Vocab()\n",
    "\n",
    "# Build models\n",
    "text_encoder = build_text_encoder(vocab_size=len(vocab), pad_id=vocab.pad_id)\n",
    "decoder = build_decoder()\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = Path(config['save_dir']) / 'ema_best.pt'\n",
    "if not ckpt_path.exists():\n",
    "    print(f\"Checkpoint not found: {ckpt_path}\")\n",
    "    print(\"Train the model first (Section 6).\")\n",
    "else:\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    text_encoder.load_state_dict(checkpoint['text_encoder'])\n",
    "    decoder.load_state_dict(checkpoint['decoder'])\n",
    "    \n",
    "    text_encoder.to(device).eval()\n",
    "    decoder.to(device).eval()\n",
    "    \n",
    "    print(f\"✓ Model loaded successfully\")\n",
    "    print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation functions\n",
    "def generate(text):\n",
    "    \"\"\"Generate image from text.\"\"\"\n",
    "    tokens = vocab.encode(text, add_special_tokens=True)\n",
    "    token_ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        e = text_encoder(token_ids, vocab.pad_id)\n",
    "        img = decoder(e)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    img = img.squeeze(0).cpu()\n",
    "    img = (img + 1.0) / 2.0  # [-1, 1] -> [0, 1]\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    return np.clip(img, 0, 1)\n",
    "\n",
    "def show(text):\n",
    "    \"\"\"Generate and display.\"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(generate(text))\n",
    "    plt.title(text, fontsize=12, pad=10)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Generation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example generations\n",
    "prompts = [\n",
    "    \"There is one red ball.\",\n",
    "    \"There are two green cubes.\",\n",
    "    \"There are three yellow blocks.\",\n",
    "    \"The blue ball is left of the red cube.\",\n",
    "    \"The green block is on the yellow ball.\",\n",
    "    \"The red cube is in front of the blue block.\",\n",
    "]\n",
    "\n",
    "print(\"Generating images from prompts...\n",
    "\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    show(prompt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771365c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation with custom prompts\n",
    "from ipywidgets import interact, Text\n",
    "\n",
    "def generate_interactive(prompt):\n",
    "    if prompt.strip():\n",
    "        show(prompt)\n",
    "    else:\n",
    "        print(\"Enter a prompt in DSL format:\")\n",
    "        print('  \"There is one red ball.\"')\n",
    "        print('  \"There are two green cubes.\"')\n",
    "        print('  \"The blue ball is left of the red cube.\"')\n",
    "\n",
    "interact(generate_interactive, \n",
    "         prompt=Text(value=\"There is one red ball.\", \n",
    "                    description=\"Prompt:\",\n",
    "                    style={'description_width': 'initial'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e41fb5",
   "metadata": {},
   "source": [
    "## 10. Counterfactual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare how text changes affect images\n",
    "def compare(prompt1, prompt2):\n",
    "    \"\"\"Compare two prompts.\"\"\"\n",
    "    img1 = generate(prompt1)\n",
    "    img2 = generate(prompt2)\n",
    "    diff = np.abs(img1 - img2)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].set_title(prompt1, fontsize=10)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img2)\n",
    "    axes[1].set_title(prompt2, fontsize=10)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(diff, cmap='hot')\n",
    "    axes[2].set_title('Difference (hotter = more change)', fontsize=10)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    l2_diff = np.mean(diff ** 2)\n",
    "    print(f\"L2 Difference: {l2_diff:.6f}\n",
    "\")\n",
    "\n",
    "# Color change\n",
    "print(\"Color Change:\")\n",
    "compare(\"There is one red ball.\", \"There is one blue ball.\")\n",
    "\n",
    "# Shape change\n",
    "print(\"Shape Change:\")\n",
    "compare(\"There is one red ball.\", \"There is one red cube.\")\n",
    "\n",
    "# Number change\n",
    "print(\"Number Change:\")\n",
    "compare(\"There are two green cubes.\", \"There are three green cubes.\")\n",
    "\n",
    "# Relation change\n",
    "print(\"Relation Change:\")\n",
    "compare(\"The blue ball is left of the red cube.\", \n",
    "        \"The blue ball is right of the red cube.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f17096",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Completed:**\n",
    "1. Data generation with DSL and renderer\n",
    "2. Model training with FiLM-conditioned decoder\n",
    "3. Evaluation with PSNR/SSIM metrics\n",
    "4. Interactive image generation\n",
    "5. Counterfactual sensitivity analysis\n",
    "\n",
    "**Key Results:**\n",
    "- Total parameters: ~11M (2.4M encoder + 8.5M decoder)\n",
    "- Target metrics: PSNR ≥ 24 dB, SSIM ≥ 0.92\n",
    "- Training time: ~6-8 hours for 100k steps\n",
    "\n",
    "**Next Steps:**\n",
    "- Phase C2: Fine-tune with caption loss only\n",
    "- Phase C3: Add adversarial losses\n",
    "- Experiment with different compositions\n",
    "\n",
    "**Data Location:**\n",
    "All outputs are saved in your working directory (Drive or local)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
