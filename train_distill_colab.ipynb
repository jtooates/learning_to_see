{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Text-to-Image Distillation Training (Colab)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jtooates/learning_to_see/blob/main/train_distill_colab.ipynb)\n\nThis notebook trains a neural network to generate 64×64 images from text captions by distilling knowledge from a procedural renderer.\n\n**What this tests:**\n- Text encoder: Converts DSL captions → 512-d embeddings\n- Image decoder: Generates 64×64 RGB images from embeddings\n- Training pipeline: Data generation, training loop, evaluation\n- Visualization: Side-by-side comparison of renderer (teacher) vs model (student)\n\n**Expected results:**\n- Model learns to generate images matching the renderer\n- PSNR ≥ 24 dB, SSIM ≥ 0.92 after training\n- Visual quality: Clear colors, shapes, and spatial relationships"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision tqdm scikit-learn matplotlib seaborn pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repository (or skip if already cloned)\nimport os\nimport sys\n\nif not os.path.exists('learning_to_see'):\n    !git clone https://github.com/jtooates/learning_to_see.git\n    %cd learning_to_see\nelse:\n    %cd learning_to_see\n    !git pull  # Get latest changes\n    \n    # If modules are already imported, restart runtime to pick up changes\n    if 'distill_c1.trainer' in sys.modules:\n        print(\"\\n⚠️  Code updated! Please restart the runtime to load the latest changes.\")\n        print(\"   Go to: Runtime → Restart runtime\")\n        print(\"   Then run all cells again from the beginning.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Optional - for persistent storage)\n",
    "\n",
    "This allows you to save checkpoints and data to your Google Drive so they persist across Colab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories in Google Drive for persistent storage\n",
    "DRIVE_DIR = '/content/drive/MyDrive/learning_to_see_colab'\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_DIR}/data', exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_DIR}/runs', exist_ok=True)\n",
    "\n",
    "print(f\"Data and checkpoints will be saved to: {DRIVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nCONFIG = {\n    # Data\n    'n_samples': 3000,  # Number of training samples (use 1000 for quick test, 6000+ for full training)\n    'data_seed': 42,\n    \n    # Training\n    'steps': 20000,  # Training steps (use 5000 for quick test, 100000 for full training)\n    'batch_size': 128,  # Reduce if OOM (try 64 or 32)\n    'lr': 1e-3,\n    \n    # Loss weights (balanced for plausible images, not perfect reconstruction)\n    'pixel_weight': 0.1,   # Low weight on pixel matching (L1+L2)\n    'tv_weight': 1e-4,     # Smoothness regularization\n    'perc_weight': 1.0,    # High weight on perceptual/structure matching\n    'use_perc': True,      # Enable perceptual loss\n    \n    'use_amp': True,  # Automatic mixed precision\n    'seed': 1337,\n    \n    # Evaluation\n    'eval_every': 1000,  # Evaluate every N steps\n    \n    # Paths (will use Google Drive if mounted, otherwise local)\n    'use_drive': os.path.exists('/content/drive/MyDrive'),\n}\n\n# Set paths based on whether Drive is mounted\nif CONFIG['use_drive']:\n    CONFIG['data_dir'] = f\"{DRIVE_DIR}/data/scenes\"\n    CONFIG['save_dir'] = f\"{DRIVE_DIR}/runs/distill_test\"\nelse:\n    CONFIG['data_dir'] = \"/content/data/scenes\"\n    CONFIG['save_dir'] = \"/content/runs/distill_test\"\n\nprint(\"Configuration:\")\nfor k, v in CONFIG.items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Data\n",
    "\n",
    "Generate scene graphs, render images, and create text captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(CONFIG['data_seed'])\n",
    "\n",
    "# Check if data already exists\n",
    "data_path = Path(CONFIG['data_dir'])\n",
    "if data_path.exists() and (data_path / 'train_shard_0.pt').exists():\n",
    "    print(f\"✓ Data already exists at {CONFIG['data_dir']}\")\n",
    "    print(\"  Skipping data generation. Delete the directory to regenerate.\")\n",
    "else:\n",
    "    print(f\"Generating {CONFIG['n_samples']} samples...\")\n",
    "    print(f\"Output directory: {CONFIG['data_dir']}\")\n",
    "    \n",
    "    # Run data generation\n",
    "    !python -m data.gen \\\n",
    "        --out_dir {CONFIG['data_dir']} \\\n",
    "        --n {CONFIG['n_samples']} \\\n",
    "        --split_strategy random \\\n",
    "        --seed {CONFIG['data_seed']}\n",
    "    \n",
    "    print(\"\\n✓ Data generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize Sample Data\n",
    "\n",
    "Let's look at some examples from the generated dataset to verify quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from data.dataset import SceneDataset\n",
    "from dsl.tokens import Vocab\n",
    "\n",
    "# Load dataset\n",
    "vocab = Vocab()\n",
    "dataset = SceneDataset(\n",
    "    data_dir=CONFIG['data_dir'],\n",
    "    split='train',\n",
    "    vocab=vocab,\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")\n",
    "print(f\"Vocabulary size: {len(vocab)} tokens\")\n",
    "\n",
    "# Visualize first 8 samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    sample = dataset[i]\n",
    "    image = sample['image']  # Shape: (3, 64, 64), range [-1, 1]\n",
    "    text = sample['text']\n",
    "    \n",
    "    # Convert to displayable format\n",
    "    img_display = (image.permute(1, 2, 0).numpy() + 1) / 2  # [-1,1] -> [0,1]\n",
    "    img_display = np.clip(img_display, 0, 1)\n",
    "    \n",
    "    axes[i].imshow(img_display)\n",
    "    axes[i].set_title(text, fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/sample_data.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Sample visualization saved to /content/sample_data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Models\n",
    "\n",
    "Build the text encoder and image decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distill_c1.text_encoder import build_text_encoder\n",
    "from distill_c1.decoder import build_decoder\n",
    "from dsl.tokens import Vocab\n",
    "\n",
    "# Initialize vocabulary\n",
    "vocab = Vocab()\n",
    "print(f\"Vocabulary: {len(vocab)} tokens\")\n",
    "print(f\"Special tokens: BOS={vocab.bos_id}, EOS={vocab.eos_id}, PAD={vocab.pad_id}\")\n",
    "\n",
    "# Build models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nDevice: {device}\")\n",
    "\n",
    "text_encoder = build_text_encoder(vocab_size=len(vocab), pad_id=vocab.pad_id).to(device)\n",
    "decoder = build_decoder().to(device)\n",
    "\n",
    "# Count parameters\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "encoder_params = count_params(text_encoder)\n",
    "decoder_params = count_params(decoder)\n",
    "total_params = encoder_params + decoder_params\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Text encoder: {encoder_params:,} ({encoder_params/1e6:.2f}M)\")\n",
    "print(f\"  Image decoder: {decoder_params:,} ({decoder_params/1e6:.2f}M)\")\n",
    "print(f\"  Total: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "test_tokens = torch.randint(1, len(vocab), (2, 20)).to(device)\n",
    "with torch.no_grad():\n",
    "    embeddings = text_encoder(test_tokens, pad_id=vocab.pad_id)\n",
    "    images = decoder(embeddings)\n",
    "\n",
    "print(f\"  Input shape: {test_tokens.shape}\")\n",
    "print(f\"  Embedding shape: {embeddings.shape}\")\n",
    "print(f\"  Output shape: {images.shape}\")\n",
    "print(f\"  Output range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(\"\\n✓ Models initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model\n",
    "\n",
    "Train the text-to-image model with distillation from the renderer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.utils.data import DataLoader\nfrom distill_c1.trainer import DistillTrainer, DistillDataset, distill_collate_fn\n\n# Set training seed\nset_seed(CONFIG['seed'])\n\n# Create datasets\ntrain_dataset = DistillDataset(\n    data_dir=CONFIG['data_dir'],\n    split='train',\n    vocab=vocab,\n)\n\nval_dataset = DistillDataset(\n    data_dir=CONFIG['data_dir'],\n    split='val',\n    vocab=vocab,\n)\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")\n\n# Create data loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n    collate_fn=distill_collate_fn,\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True,\n    collate_fn=distill_collate_fn,\n)\n\n# Create trainer\ntrainer = DistillTrainer(\n    text_encoder=text_encoder,\n    decoder=decoder,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    vocab=vocab,\n    device=device,\n    lr=CONFIG['lr'],\n    save_dir=CONFIG['save_dir'],\n    pixel_weight=CONFIG['pixel_weight'],\n    tv_weight=CONFIG['tv_weight'],\n    perc_weight=CONFIG['perc_weight'],\n    use_perc=CONFIG['use_perc'],\n    use_amp=CONFIG['use_amp'],\n    eval_every=CONFIG['eval_every'],\n)\n\nprint(f\"\\nStarting training for {CONFIG['steps']} steps...\")\nprint(f\"Checkpoints will be saved to: {CONFIG['save_dir']}\")\nprint(\"\\nTraining metrics will be displayed below:\")\nprint(\"-\" * 80)\n\n# Train - pass total_steps to the train() method\ntrainer.train(total_steps=CONFIG['steps'])\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"✓ Training complete!\")\nprint(f\"  Best checkpoint: {CONFIG['save_dir']}/ema_best.pt\")\nprint(f\"  Latest checkpoint: {CONFIG['save_dir']}/last.pt\")\nprint(f\"  Training log: {CONFIG['save_dir']}/log.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Training Results\n",
    "\n",
    "Load and display the generated images during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from PIL import Image\nfrom pathlib import Path\nimport json\n\nsave_dir = Path(CONFIG['save_dir'])\nsamples_dir = save_dir / 'samples'\n\n# Display training curve\nlog_path = save_dir / 'log.json'\nif log_path.exists():\n    with open(log_path, 'r') as f:\n        log_data = json.load(f)  # Load as JSON array, not line-by-line\n    \n    # Extract metrics\n    steps = [entry['step'] for entry in log_data]\n    psnr_vals = [entry['metrics']['psnr'] for entry in log_data]\n    ssim_vals = [entry['metrics']['ssim'] for entry in log_data]\n    loss_vals = [entry['metrics']['total'] for entry in log_data]\n    \n    # Plot\n    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n    \n    axes[0].plot(steps, loss_vals, marker='o')\n    axes[0].set_xlabel('Step')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Validation Loss')\n    axes[0].grid(True, alpha=0.3)\n    \n    axes[1].plot(steps, psnr_vals, marker='o')\n    axes[1].axhline(y=24, color='r', linestyle='--', alpha=0.5, label='Target: 24 dB')\n    axes[1].set_xlabel('Step')\n    axes[1].set_ylabel('PSNR (dB)')\n    axes[1].set_title('Validation PSNR')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    axes[2].plot(steps, ssim_vals, marker='o')\n    axes[2].axhline(y=0.92, color='r', linestyle='--', alpha=0.5, label='Target: 0.92')\n    axes[2].set_xlabel('Step')\n    axes[2].set_ylabel('SSIM')\n    axes[2].set_title('Validation SSIM')\n    axes[2].legend()\n    axes[2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig('/content/training_curves.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # Print final metrics\n    if psnr_vals:\n        print(f\"\\nFinal validation metrics:\")\n        print(f\"  PSNR: {psnr_vals[-1]:.2f} dB (target: ≥24 dB)\")\n        print(f\"  SSIM: {ssim_vals[-1]:.4f} (target: ≥0.92)\")\n        \n        if psnr_vals[-1] >= 24 and ssim_vals[-1] >= 0.92:\n            print(\"\\n✓ Model meets acceptance criteria!\")\n        else:\n            print(\"\\n⚠ Model needs more training to meet acceptance criteria.\")\n            print(\"  Consider increasing 'steps' in CONFIG and re-running training.\")\n\n# Display latest generated images\nif samples_dir.exists():\n    sample_files = sorted(samples_dir.glob('step_*.png'))\n    if sample_files:\n        latest_sample = sample_files[-1]\n        print(f\"\\nLatest generated samples ({latest_sample.name}):\")\n        img = Image.open(latest_sample)\n        plt.figure(figsize=(14, 10))\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title('Generated Images (Top: Teacher/Renderer, Bottom: Student/Model)', fontsize=14)\n        plt.tight_layout()\n        plt.savefig('/content/latest_samples.png', dpi=150, bbox_inches='tight')\n        plt.show()\n        print(\"\\nNote: Top row = Teacher (renderer), Bottom row = Student (model)\")\n        print(\"The model should learn to match the renderer's output.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the Trained Model\n",
    "\n",
    "Generate images from custom text captions to verify the model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint_path = Path(CONFIG['save_dir']) / 'ema_best.pt'\n",
    "\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    text_encoder.load_state_dict(checkpoint['text_encoder'])\n",
    "    decoder.load_state_dict(checkpoint['decoder'])\n",
    "    text_encoder.eval()\n",
    "    decoder.eval()\n",
    "    print(\"✓ Model loaded successfully\")\n",
    "else:\n",
    "    print(f\"⚠ Checkpoint not found: {checkpoint_path}\")\n",
    "    print(\"Using current model state (may not be optimal)\")\n",
    "\n",
    "# Test captions\n",
    "test_captions = [\n",
    "    \"There is one red ball.\",\n",
    "    \"There are three blue cubes.\",\n",
    "    \"There are five yellow blocks.\",\n",
    "    \"The red ball is left of the blue cube.\",\n",
    "    \"The green block is on the yellow ball.\",\n",
    "    \"The blue cube is in front of the red block.\",\n",
    "    \"There are two green balls.\",\n",
    "    \"There are four red blocks.\",\n",
    "]\n",
    "\n",
    "print(f\"\\nGenerating images for {len(test_captions)} test captions...\")\n",
    "\n",
    "# Generate images\n",
    "generated_images = []\n",
    "with torch.no_grad():\n",
    "    for caption in test_captions:\n",
    "        # Tokenize\n",
    "        token_ids = vocab.encode(caption, add_special=True)\n",
    "        token_ids = torch.tensor(token_ids).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "        \n",
    "        # Generate image\n",
    "        embedding = text_encoder(token_ids, pad_id=vocab.pad_id)\n",
    "        image = decoder(embedding)\n",
    "        \n",
    "        # Convert to displayable format\n",
    "        image = image.cpu().squeeze(0)  # (3, 64, 64)\n",
    "        image = (image.permute(1, 2, 0).numpy() + 1) / 2  # [-1,1] -> [0,1]\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "        generated_images.append(image)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (caption, image) in enumerate(zip(test_captions, generated_images)):\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(caption, fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/test_generations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Test generation complete!\")\n",
    "print(\"\\nWhat to look for:\")\n",
    "print(\"  ✓ Correct colors (red, blue, green, yellow)\")\n",
    "print(\"  ✓ Correct shapes (ball=circle, cube/block=square)\")\n",
    "print(\"  ✓ Correct counts (one, two, three, four, five)\")\n",
    "print(\"  ✓ Correct spatial relations (left of, on, in front of)\")\n",
    "print(\"  ✓ No checkerboard artifacts or blurriness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare with Ground Truth (Renderer)\n",
    "\n",
    "Generate side-by-side comparisons with the renderer to evaluate quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsl.parser import SceneParser\n",
    "from render.renderer import SceneRenderer\n",
    "\n",
    "# Initialize renderer\n",
    "parser = SceneParser()\n",
    "renderer = SceneRenderer(seed=42)\n",
    "\n",
    "print(\"Generating comparisons with ground truth renderer...\\n\")\n",
    "\n",
    "# Use subset of test captions\n",
    "comparison_captions = test_captions[:4]\n",
    "\n",
    "fig, axes = plt.subplots(len(comparison_captions), 3, figsize=(12, 4 * len(comparison_captions)))\n",
    "if len(comparison_captions) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, caption in enumerate(comparison_captions):\n",
    "        # Generate with renderer (teacher)\n",
    "        scene_graph = parser.parse(caption)\n",
    "        teacher_image, _ = renderer.render(scene_graph)\n",
    "        teacher_array = np.array(teacher_image) / 255.0  # [0, 1]\n",
    "        \n",
    "        # Generate with model (student)\n",
    "        token_ids = vocab.encode(caption, add_special=True)\n",
    "        token_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
    "        embedding = text_encoder(token_ids, pad_id=vocab.pad_id)\n",
    "        student_image = decoder(embedding)\n",
    "        student_array = student_image.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "        student_array = (student_array + 1) / 2  # [-1,1] -> [0,1]\n",
    "        student_array = np.clip(student_array, 0, 1)\n",
    "        \n",
    "        # Compute difference\n",
    "        diff = np.abs(teacher_array - student_array)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i, 0].imshow(teacher_array)\n",
    "        axes[i, 0].set_title(f'Teacher (Renderer)\\n{caption}', fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(student_array)\n",
    "        axes[i, 1].set_title(f'Student (Model)\\n{caption}', fontsize=10)\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(diff, cmap='hot', vmin=0, vmax=1)\n",
    "        axes[i, 2].set_title(f'Absolute Difference\\nMean: {diff.mean():.4f}', fontsize=10)\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/teacher_student_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Comparison complete!\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  • Left column: Ground truth from procedural renderer\")\n",
    "print(\"  • Middle column: Generated by trained model\")\n",
    "print(\"  • Right column: Pixel-wise difference (red = high error)\")\n",
    "print(\"\\nThe model should closely match the renderer with minimal difference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested the text-to-image distillation pipeline:\n",
    "\n",
    "1. ✓ Generated synthetic data (scenes + captions + images)\n",
    "2. ✓ Trained text encoder + image decoder\n",
    "3. ✓ Visualized training progress\n",
    "4. ✓ Generated images from test captions\n",
    "5. ✓ Compared model output with ground truth renderer\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**If results look good:**\n",
    "- Train on more data (increase `n_samples` to 6000+)\n",
    "- Train longer (increase `steps` to 100,000)\n",
    "- Evaluate on compositional generalization (holdout splits)\n",
    "\n",
    "**If results need improvement:**\n",
    "- Check data quality (visualizations in Step 2)\n",
    "- Verify metrics (PSNR ≥ 24 dB, SSIM ≥ 0.92)\n",
    "- Adjust hyperparameters (learning rate, loss weights)\n",
    "\n",
    "**Files saved:**\n",
    "- `/content/sample_data.png` - Training data samples\n",
    "- `/content/training_curves.png` - Training metrics over time\n",
    "- `/content/latest_samples.png` - Generated samples during training\n",
    "- `/content/test_generations.png` - Model outputs on test captions\n",
    "- `/content/teacher_student_comparison.png` - Side-by-side comparison\n",
    "\n",
    "All checkpoints and logs are saved to your specified save directory (Google Drive if mounted)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}