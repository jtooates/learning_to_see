{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Text-to-Image Distillation Training (C1: Renderer Distillation)\n",
    "\n",
    "This notebook trains a neural network to generate 64×64 images from text captions by distilling knowledge from a procedural renderer.\n",
    "\n",
    "**Architecture:**\n",
    "- Text Encoder: 4-layer Transformer (2.4M params)\n",
    "- Image Decoder: FiLM-conditioned CNN with ResBlocks (8.5M params)\n",
    "- Training: Pixel losses + TV + random perceptual loss\n",
    "\n",
    "**Expected Results:**\n",
    "- PSNR: 26-28 dB\n",
    "- SSIM: 0.93-0.95\n",
    "- Training time: ~6-8 hours for 100k steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone repository (if not already cloned)\n",
    "import os\n",
    "if not os.path.exists('learning_to_see'):\n",
    "    !git clone https://github.com/YOUR_USERNAME/learning_to_see.git\n",
    "    %cd learning_to_see\n",
    "else:\n",
    "    %cd learning_to_see\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q pillow numpy regex tqdm pyyaml matplotlib ipywidgets\n",
    "\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-header"
   },
   "source": [
    "## 2. Generate Training Data\n",
    "\n",
    "Generate synthetic scene images with captions using the DSL and procedural renderer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-data"
   },
   "outputs": [],
   "source": [
    "# Generate data (adjust --n based on desired dataset size)\n",
    "# For quick testing: --n 1000\n",
    "# For full training: --n 6000\n",
    "\n",
    "!python -m data.gen \\\n",
    "  --out_dir data/scenes \\\n",
    "  --n 3000 \\\n",
    "  --split_strategy random \\\n",
    "  --seed 42\n",
    "\n",
    "print(\"\\n✓ Data generation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-data"
   },
   "outputs": [],
   "source": [
    "# Visualize sample data\n",
    "!python visualize_samples.py --data_dir data/scenes --n 16 --save_path samples_preview.png\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image('samples_preview.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-header"
   },
   "source": [
    "## 3. Run Tests\n",
    "\n",
    "Verify that all components work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-tests"
   },
   "outputs": [],
   "source": [
    "# Run unit tests\n",
    "!python -m pytest distill_c1/tests_distill.py -v --tb=short\n",
    "\n",
    "print(\"\\n✓ All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train-header"
   },
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "Configure training parameters. Adjust based on your needs and available compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    # Data\n",
    "    'data_dir': 'data/scenes',\n",
    "    'save_dir': 'runs/distill_c1_colab',\n",
    "    \n",
    "    # Training schedule\n",
    "    'steps': 50000,      # Reduce for faster training (full: 100000)\n",
    "    'batch': 128,        # Adjust based on GPU memory (192 for A100, 64 for T4)\n",
    "    'eval_every': 2000,\n",
    "    \n",
    "    # Optimizer\n",
    "    'lr': 3e-4,\n",
    "    'wd': 0.05,\n",
    "    'warmup': 1000,\n",
    "    'grad_clip': 1.0,\n",
    "    \n",
    "    # Loss weights\n",
    "    'tv': 1e-5,\n",
    "    'perc': 1e-3,\n",
    "    'use_perc': True,\n",
    "    \n",
    "    # Model\n",
    "    'emb_dim': 512,\n",
    "    'base_ch': 256,\n",
    "    'attn_heads': 4,\n",
    "    \n",
    "    # Training\n",
    "    'use_amp': True,\n",
    "    'ema': 0.999,\n",
    "    'num_workers': 2,\n",
    "    'seed': 1337,\n",
    "}\n",
    "\n",
    "# Display config\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in config.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train-quick-header"
   },
   "source": [
    "## 5. Quick Sanity Check (Optional)\n",
    "\n",
    "Run a very short training run to verify everything works before the full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick-train"
   },
   "outputs": [],
   "source": [
    "# Quick sanity check (500 steps, ~5 minutes)\n",
    "!python -m distill_c1.train_distill \\\n",
    "  --data_dir data/scenes \\\n",
    "  --save_dir runs/sanity_check \\\n",
    "  --steps 500 \\\n",
    "  --batch 64 \\\n",
    "  --eval_every 250 \\\n",
    "  --seed 42\n",
    "\n",
    "print(\"\\n✓ Sanity check complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train-full-header"
   },
   "source": [
    "## 6. Full Training\n",
    "\n",
    "Train the full model. This will take several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-full"
   },
   "outputs": [],
   "source": [
    "# Build training command\n",
    "train_cmd = f\"\"\"\n",
    "python -m distill_c1.train_distill \\\n",
    "  --data_dir {config['data_dir']} \\\n",
    "  --save_dir {config['save_dir']} \\\n",
    "  --steps {config['steps']} \\\n",
    "  --batch {config['batch']} \\\n",
    "  --lr {config['lr']} \\\n",
    "  --wd {config['wd']} \\\n",
    "  --warmup {config['warmup']} \\\n",
    "  --grad_clip {config['grad_clip']} \\\n",
    "  --tv {config['tv']} \\\n",
    "  --perc {config['perc']} \\\n",
    "  --eval_every {config['eval_every']} \\\n",
    "  --emb_dim {config['emb_dim']} \\\n",
    "  --base_ch {config['base_ch']} \\\n",
    "  --attn_heads {config['attn_heads']} \\\n",
    "  --ema {config['ema']} \\\n",
    "  --num_workers {config['num_workers']} \\\n",
    "  --seed {config['seed']}\n",
    "\"\"\"\n",
    "\n",
    "if config['use_amp']:\n",
    "    train_cmd += \" --use_amp\"\n",
    "if not config['use_perc']:\n",
    "    train_cmd += \" --no_perc\"\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Expected time: ~{config['steps'] / 12000:.1f} hours\\n\")\n",
    "\n",
    "!{train_cmd}\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize-training-header"
   },
   "source": [
    "## 7. Visualize Training Progress\n",
    "\n",
    "Plot training metrics and view generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-metrics"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load training log\n",
    "log_path = Path(config['save_dir']) / 'log.json'\n",
    "with open(log_path, 'r') as f:\n",
    "    log = json.load(f)\n",
    "\n",
    "# Extract metrics\n",
    "steps = [entry['step'] for entry in log]\n",
    "psnr = [entry['metrics']['psnr'] for entry in log]\n",
    "ssim = [entry['metrics']['ssim'] for entry in log]\n",
    "loss = [entry['metrics']['total'] for entry in log]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# PSNR\n",
    "axes[0].plot(steps, psnr, linewidth=2)\n",
    "axes[0].axhline(y=24, color='r', linestyle='--', label='Target (24 dB)')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('PSNR (dB)')\n",
    "axes[0].set_title('Peak Signal-to-Noise Ratio')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# SSIM\n",
    "axes[1].plot(steps, ssim, linewidth=2, color='orange')\n",
    "axes[1].axhline(y=0.92, color='r', linestyle='--', label='Target (0.92)')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('SSIM')\n",
    "axes[1].set_title('Structural Similarity Index')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[2].plot(steps, loss, linewidth=2, color='green')\n",
    "axes[2].set_xlabel('Step')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_title('Training Loss')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  PSNR: {psnr[-1]:.2f} dB\")\n",
    "print(f\"  SSIM: {ssim[-1]:.4f}\")\n",
    "print(f\"  Loss: {loss[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view-samples"
   },
   "outputs": [],
   "source": [
    "# View generated samples from different training steps\n",
    "from IPython.display import Image, display\n",
    "import glob\n",
    "\n",
    "sample_dir = Path(config['save_dir']) / 'samples'\n",
    "sample_files = sorted(glob.glob(str(sample_dir / 'step_*.png')))\n",
    "\n",
    "print(\"Generated Samples During Training:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show samples at different checkpoints\n",
    "indices = [0, len(sample_files)//4, len(sample_files)//2, 3*len(sample_files)//4, -1]\n",
    "for idx in indices:\n",
    "    if 0 <= idx < len(sample_files) or idx == -1:\n",
    "        file = sample_files[idx]\n",
    "        step = Path(file).stem.split('_')[1]\n",
    "        print(f\"\\nStep {step}:\")\n",
    "        display(Image(file, width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eval-header"
   },
   "source": [
    "## 8. Evaluation\n",
    "\n",
    "Evaluate the trained model on the validation set with comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "eval_cmd = f\"\"\"\n",
    "python -m distill_c1.eval_distill \\\n",
    "  --data_dir {config['data_dir']} \\\n",
    "  --ckpt {config['save_dir']}/ema_best.pt \\\n",
    "  --report {config['save_dir']}/report.json \\\n",
    "  --save_images {config['save_dir']}/eval_images \\\n",
    "  --split val \\\n",
    "  --counterfactual\n",
    "\"\"\"\n",
    "\n",
    "!{eval_cmd}\n",
    "\n",
    "print(\"\\n✓ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-eval-report"
   },
   "outputs": [],
   "source": [
    "# Display evaluation report\n",
    "report_path = Path(config['save_dir']) / 'report.json'\n",
    "with open(report_path, 'r') as f:\n",
    "    report = json.load(f)\n",
    "\n",
    "print(\"Evaluation Report\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Split: {report['split']}\")\n",
    "print(f\"Samples: {report['num_samples']}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  PSNR: {report['metrics']['psnr']:.2f} dB\")\n",
    "print(f\"  SSIM: {report['metrics']['ssim']:.4f}\")\n",
    "\n",
    "# Acceptance criteria\n",
    "psnr_pass = report['metrics']['psnr'] >= 24.0\n",
    "ssim_pass = report['metrics']['ssim'] >= 0.92\n",
    "\n",
    "print(f\"\\nAcceptance Criteria:\")\n",
    "print(f\"  PSNR ≥ 24 dB: {'✓ PASS' if psnr_pass else '✗ FAIL'}\")\n",
    "print(f\"  SSIM ≥ 0.92:  {'✓ PASS' if ssim_pass else '✗ FAIL'}\")\n",
    "\n",
    "if 'counterfactual' in report and report['counterfactual']:\n",
    "    print(f\"\\nCounterfactual Sensitivity:\")\n",
    "    for edit_type, results in report['counterfactual'].items():\n",
    "        print(f\"  {edit_type}: ΔL2 = {results['avg_delta_l2']:.6f}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-eval-images"
   },
   "outputs": [],
   "source": [
    "# Display evaluation visualizations\n",
    "eval_img_dir = Path(config['save_dir']) / 'eval_images'\n",
    "\n",
    "print(\"Teacher vs Student (3 rows: Teacher, Student, Difference):\")\n",
    "display(Image(str(eval_img_dir / 'grid.png'), width=900))\n",
    "\n",
    "print(\"\\nSide-by-Side Comparison:\")\n",
    "display(Image(str(eval_img_dir / 'comparison.png'), width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "interactive-header"
   },
   "source": [
    "## 9. Interactive Generation\n",
    "\n",
    "Generate images from custom text prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "import torch\n",
    "from distill_c1.text_encoder import build_text_encoder\n",
    "from distill_c1.decoder import build_decoder\n",
    "from dsl.tokens import Vocab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load vocabulary\n",
    "vocab = Vocab()\n",
    "\n",
    "# Build models\n",
    "text_encoder = build_text_encoder(vocab_size=len(vocab), pad_id=vocab.pad_id)\n",
    "decoder = build_decoder()\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = Path(config['save_dir']) / 'ema_best.pt'\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "text_encoder.load_state_dict(checkpoint['text_encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "\n",
    "text_encoder = text_encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "text_encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(\"✓ Model loaded successfully\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-function"
   },
   "outputs": [],
   "source": [
    "def generate_image(text: str):\n",
    "    \"\"\"Generate image from text prompt.\"\"\"\n",
    "    # Tokenize\n",
    "    tokens = vocab.encode(text, add_special_tokens=True)\n",
    "    token_ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        e = text_encoder(token_ids, pad_id=vocab.pad_id)\n",
    "        img = decoder(e)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    img = img.squeeze(0).cpu()\n",
    "    img = (img + 1.0) / 2.0  # [-1, 1] -> [0, 1]\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def visualize_generation(text: str):\n",
    "    \"\"\"Generate and display image.\"\"\"\n",
    "    img = generate_image(text)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(text, fontsize=14, pad=10)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Generation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example-generations"
   },
   "outputs": [],
   "source": [
    "# Example generations\n",
    "prompts = [\n",
    "    \"There is one red ball.\",\n",
    "    \"There are two green cubes.\",\n",
    "    \"There are three yellow blocks.\",\n",
    "    \"The blue ball is left of the red cube.\",\n",
    "    \"The green block is on the yellow ball.\",\n",
    "    \"The red cube is in front of the blue block.\",\n",
    "    \"There are five red balls.\",\n",
    "    \"The yellow cube is right of the green ball.\",\n",
    "]\n",
    "\n",
    "print(\"Generating images from prompts...\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    visualize_generation(prompt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom-prompt"
   },
   "outputs": [],
   "source": [
    "# Interactive: Generate from custom prompt\n",
    "from ipywidgets import interact, Text\n",
    "\n",
    "def generate_interactive(prompt):\n",
    "    if prompt.strip():\n",
    "        visualize_generation(prompt)\n",
    "    else:\n",
    "        print(\"Please enter a prompt in DSL format.\")\n",
    "        print(\"Examples:\")\n",
    "        print('  \"There is one red ball.\"')\n",
    "        print('  \"There are two green cubes.\"')\n",
    "        print('  \"The blue ball is left of the red cube.\"')\n",
    "\n",
    "interact(generate_interactive, prompt=Text(value=\"There is one red ball.\", description=\"Prompt:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "counterfactual-header"
   },
   "source": [
    "## 10. Counterfactual Analysis\n",
    "\n",
    "Visualize how small changes in text affect the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "counterfactual-vis"
   },
   "outputs": [],
   "source": [
    "def compare_prompts(prompt1: str, prompt2: str):\n",
    "    \"\"\"Compare images from two prompts.\"\"\"\n",
    "    img1 = generate_image(prompt1)\n",
    "    img2 = generate_image(prompt2)\n",
    "    diff = np.abs(img1 - img2)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(img1)\n",
    "    axes[0].set_title(prompt1, fontsize=10)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(img2)\n",
    "    axes[1].set_title(prompt2, fontsize=10)\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(diff, cmap='hot')\n",
    "    axes[2].set_title('Difference (hotter = more change)', fontsize=10)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute metrics\n",
    "    l2_diff = np.mean(diff ** 2)\n",
    "    print(f\"L2 Difference: {l2_diff:.6f}\")\n",
    "\n",
    "# Color changes\n",
    "print(\"Color Change:\")\n",
    "compare_prompts(\n",
    "    \"There is one red ball.\",\n",
    "    \"There is one blue ball.\"\n",
    ")\n",
    "\n",
    "# Shape changes\n",
    "print(\"\\nShape Change:\")\n",
    "compare_prompts(\n",
    "    \"There is one red ball.\",\n",
    "    \"There is one red cube.\"\n",
    ")\n",
    "\n",
    "# Number changes\n",
    "print(\"\\nNumber Change:\")\n",
    "compare_prompts(\n",
    "    \"There are two green cubes.\",\n",
    "    \"There are three green cubes.\"\n",
    ")\n",
    "\n",
    "# Relation changes\n",
    "print(\"\\nRelation Change:\")\n",
    "compare_prompts(\n",
    "    \"The blue ball is left of the red cube.\",\n",
    "    \"The blue ball is right of the red cube.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-header"
   },
   "source": [
    "## 11. Download Results\n",
    "\n",
    "Download trained models and results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-files"
   },
   "outputs": [],
   "source": [
    "# Create zip file with results\n",
    "import shutil\n",
    "\n",
    "# Files to include\n",
    "save_dir = Path(config['save_dir'])\n",
    "zip_name = 'distill_c1_results'\n",
    "\n",
    "# Create archive\n",
    "shutil.make_archive(zip_name, 'zip', save_dir)\n",
    "\n",
    "print(f\"✓ Created {zip_name}.zip\")\n",
    "print(f\"\\nContents:\")\n",
    "print(f\"  - best.pt (best training checkpoint)\")\n",
    "print(f\"  - ema_best.pt (best EMA checkpoint - recommended)\")\n",
    "print(f\"  - last.pt (latest checkpoint)\")\n",
    "print(f\"  - log.json (training metrics)\")\n",
    "print(f\"  - report.json (evaluation results)\")\n",
    "print(f\"  - samples/ (generated samples during training)\")\n",
    "print(f\"  - eval_images/ (evaluation visualizations)\")\n",
    "\n",
    "# Download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(f'{zip_name}.zip')\n",
    "    print(f\"\\n✓ Download started\")\n",
    "except:\n",
    "    print(f\"\\n✓ Zip file ready at: {zip_name}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-header"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ Data generation from DSL and procedural renderer\n",
    "2. ✅ Model training with FiLM-conditioned decoder\n",
    "3. ✅ Training visualization and metrics tracking\n",
    "4. ✅ Model evaluation with PSNR/SSIM\n",
    "5. ✅ Interactive image generation from text\n",
    "6. ✅ Counterfactual sensitivity analysis\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Phase C2**: Fine-tune with caption loss only (strict regime)\n",
    "- **Phase C3**: Add adversarial losses for realism\n",
    "- **Experiment**: Try different DSL compositions\n",
    "- **Scale up**: Train on larger datasets or longer\n",
    "\n",
    "### Key Results\n",
    "\n",
    "- Total parameters: ~11M (2.4M encoder + 8.5M decoder)\n",
    "- Expected PSNR: 26-28 dB (target: ≥24 dB)\n",
    "- Expected SSIM: 0.93-0.95 (target: ≥0.92)\n",
    "- Training time: ~6-8 hours for 100k steps on GPU"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
